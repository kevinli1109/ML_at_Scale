{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final: Criteo Display Advertising Challenge\n",
    "### Shan He, Tako Hisada, Daniel Kent, Steve Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2014, Criteo Labs, a Paris-based global technology company, launched a Kaggle Competition that encouraged Kagglers to submit the most accurate ML algorithm for Click Through Rate Label Estimation.\n",
    "\n",
    "Started in 2005, Criteo, which has raised over 60m USD (https://www.crunchbase.com/organization/criteo) in venture funding before going public in 2013, creates a digital online display advertising service that helps ecommerce companies and brands \"acquire, convert and re-engage their customers, using shopping data, predictive technology and large consumer reach.\" The company collects data from users and pools the data to additionally offer insights into customer purchasing habits and intent. It further leverages machine learning on its proprietary data, serving digital advertisements across different channels. (https://www.bamsec.com/filing/157642718000008?cik=1576427)\n",
    "\n",
    "The company collects data on over 600b USD worth of ecommerce transactions and has delivered over 10b targeted clicks in 2017. From those 10b clicks, Criteo's clients generated approximately 29b USD in post-click sales. (https://www.bamsec.com/filing/157642718000008?cik=1576427)\n",
    "\n",
    "The company offers their digital advertisements on a cost-per-click basis along with follow-up data that ties to post-click sales, facilitating a click-to-purchase Cost-of-Acquisition Metric available to its customers. (https://www.bamsec.com/filing/157642718000008?cik=1576427)\n",
    "\n",
    "The reason for Criteo to support a Kaggle competition is clear as it solicited machine learning models that could then be adapted for its internal algorithms which could help the company gain a greater profit margin by differentiating itself from its competition and providing better targeting services. It provided a training and test set of its internal proprietary data and 718 individual teams competed for the 16,000 USD prize amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a component of our w261: Machine Learning at Scale Class, **our team is looking at the data to create a scalable machine learning algorithm to correctly predict CTR**.\n",
    "\n",
    "Understanding the drivers of what influences an individual user to click on a link or to not click on a link, has billions of dollars worth of implications. The present display and search engine advertising markets would seek to uncover attributes of individuals with data that is easily collected to display advertisements that have a high probability of the user clicking on the advertisement, resulting in the advertising platform getting paid a commission from the advertiser who created the ad. If we are able to develop a model that predicts user click through rates, the digital advertising market could become more efficient as billions of dollars is wasted annually on fraud, mistargeted users, and other issues. Search engine marketers and analysts frequently perform this type of analysis to reduce one of the most expensive elements of eCommerce, which is called the Cost of Acquisition, or CAC. Particularly for SaaS and digital goods and services, CAC can constitute the majority of costs of goods sold. By being able to target interested customers better, thereby reducing the CAC of a product, less ad-spend goes wasted and the profit margins on each unit sold increases. Consequently businesses have a strong interest in ensuring that ads are properly targeted and CTRs are high.\n",
    "\n",
    "It should be noted that even though a high CTR is important, it is just one factor in a chain of others that lead to a sale - other downstream conversion efforts are critical to closing a customer's sale.\n",
    "\n",
    "The most robust ad models might include very specific customer segmentation demographics, psychographics, and past purchase and usage behavior. In this exercise, we are looking at a more limited data set, with the particular challenge of not knowing the representation and significance of our columns. This presents difficulties for the investigation team, as we can guess as to what the fields potentially represent, but we cannot use outside knowledge to help us develop our models. Consequently, we will attempt to understand what types of data we are dealing with what are the shortcomings of our data - whether it's incomplete, erroneous, or includes omissions and NaN values.\n",
    "\n",
    "In an ideal world with perfect information, we would be able to serve up advertisements that have a 100% CTR; that is, each ad we serve ups results in a click. This would then enable us to focus on conversion tactics that follow the initial ad click. According to WordStream, an online advertising consultancy, a CTR of over 2% for a search advertisement would be terrific. Typically, CTRs are divided up by industry, as each industry's customer has a different profile. For Display Advertisements, which is what Criteo primarily sells, a CTR of above 0.4% would be very good, again, depending on industry. (https://www.wordstream.com/average-ctr)\n",
    "\n",
    "For our model to be practically useful, we would want to develop a model for Criteo that beats the the average CTR in our industry. This would translate to dollars saved which could be reinvested in other ad targeting which would contribute to reducing our CAC, which would lead to greater profit margins, leading to greater benefit to shareholders. Internally, for our model to be practically useful, we would want to see any improvement over our current models that is greater than that of random chance, that allows us to target users and get a CTR higher than our current benchmarks. Ideally, the more simple the algorithm, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Discussion of Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment & Data Preparation\n",
    "As previously mentioned in our question formulation section, the dataset we are given for this exercise has no column labels, meaning we cannot leverage any of our pre-existing knowledge about how online ads are served and CTR is computed in understanding the data. This means we have to put in extra effort in analyzing the data so we can understand the relationships between different features in the dataset and process them appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data: Categorical EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data: Numerical EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addressing our project question of correctly predicting CTR, we evaluated the below set of algorithms. We attempted this by conducting research on the algorithms themselves and their use cases and implementing them with our sample datasets of 100 and 100,000 records out of the 45M+ record training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization / Factorization Machine\n",
    "\n",
    "We looked at Matrix Factorization and Factorization Machine algorithms for their ability to condense the features present in the dataset into a set of K latent features and hence will allow us to process the data easily at scale. We also discovered that some of the past winners and highly-ranked participants of the Kaggle CTR prediction competitions used the Factorization Machine algorithm and we were genuinely curious about how it worked.\n",
    "\n",
    "We made an attempt at Matrix Factorization which is a prerequisite of Factorization Machine by implementing non-parallelized implementation of the algorithm. The algorithm was designed to produce 2 matrices P and Q in the end with the dimensions <M, K> and <N, K> respectively. Multiplied together, the 2 much smaller matrices should produce an R_hat matrix that closely resembles the original R matrix.\n",
    "\n",
    "$$r_{ij} = p_i \\cdot q_j^T$$\n",
    "\n",
    "We can then compute the error as follow:\n",
    "\n",
    "$$e^2_{ij} = (r_{ij} - \\hat{r}_{ij})^2 = (r_{ij} - \\sum_{k=1}^K p_{ik}q_{kj})^2$$\n",
    "\n",
    "We compute the gradient for updating $p_{ik}$ and $q_{kj}$:\n",
    "\n",
    "$$\\frac{\\delta}{\\delta p_{ik}} e^2_{ij} = -2(r_{ij} - \\hat{r}_{ij})(q_{kj}) = -2e_{ij}q_{kj}$$\n",
    "$$\\frac{\\delta}{\\delta q_{ik}} e^2_{ij} = -2(r_{ij} - \\hat{r}_{ij})(p_{ik}) = -2e_{ij}p_{ik}$$\n",
    "\n",
    "Now we can formulate our update rules for $p_{ik}$ and $q_{kj}$:\n",
    "\n",
    "$$p'_{ik} = p_{ik} + \\alpha \\frac{\\delta}{\\delta p_{ik}} e^2_{ij} = p_{ik} + 2 \\alpha e_{ij} q_{kj}$$\n",
    "$$q'_{kj} = q_{kj} + \\alpha \\frac{\\delta}{\\delta q_{kj}} e^2_{ij} = q_{kj} + 2 \\alpha e_{ij} p_{jk}$$\n",
    "\n",
    "We ran this with the sample datasets of 100 and 100000 records respectively for varying number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02, verbose=True, debug=False):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        R     : a matrix to be factorized, dimension N x M\n",
    "        P     : an initial matrix of dimension N x K\n",
    "        Q     : an initial matrix of dimension M x K\n",
    "        K     : the number of latent features\n",
    "        steps : the maximum number of steps to perform the optimisation\n",
    "        alpha : the learning rate\n",
    "        beta  : the regularization parameter\n",
    "    returns:\n",
    "        the final matrices P and Q\n",
    "    \"\"\"\n",
    "    Q = Q.T\n",
    "    \n",
    "    start = time.time()\n",
    "    for step in range(steps):\n",
    "        \n",
    "        if verbose:\n",
    "            step_start = time.time()\n",
    "            print(f'Beginning step {step+1} ...')\n",
    "\n",
    "        for i in range(len(R)):\n",
    "            for position, j in enumerate(R[i][0].indices):\n",
    "                #TODO(thisada): Look into why squaring the error doesn't yield better results\n",
    "                eij = (R[i][0].values[position] - np.dot(P[i,:],Q[:,j]))**2\n",
    "                \n",
    "                if debug:\n",
    "                    print(f'Step {step+1} R[{i}][{j}] - position - {position}')\n",
    "\n",
    "                eij = (R[i][0].values[position] - np.dot(P[i,:],Q[:,j]))\n",
    "\n",
    "                if debug:\n",
    "                    print(f'EIJ at R[{i}][{j}]: {eij} = ({R[i][0].values[position]} - {np.dot(P[i,:],Q[:,j])})')\n",
    "\n",
    "                for k in range(K):\n",
    "                    P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                    Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "        #eR = np.dot(P, Q)\n",
    "        e = 0\n",
    "        for i in range(len(R)):\n",
    "            for position, j in enumerate(R[i][0].indices):\n",
    "                e = e + pow(R[i][0].values[position] - np.dot(P[i,:],Q[:,j]), 2)\n",
    "                for k in range(K):\n",
    "                    e = e + (beta/2) * ( pow(P[i][k],2) + pow(Q[k][j],2) )\n",
    "        if verbose:\n",
    "            print(f'Step {step+1} completed in {time.time() - step_start} seconds - ERROR: {e}')\n",
    "\n",
    "        if e < 0.001:\n",
    "            print(f'Convergence achieved at {e} after {time.time() - start} seconds. Exiting ...')\n",
    "            break\n",
    "\n",
    "    print(f'{steps} steps completed at {time.time() - start} seconds. Final error: {e}')    \n",
    "\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran this with the toy sample containing 100 records for 10000 iterations which yielded the error of 20613178.730349187."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df = df_toy.select('scaled')\n",
    "R_rows = R_df.collect()\n",
    "R_vector = R_rows[0].scaled\n",
    "                               \n",
    "N = R_df.count()\n",
    "M = R_vector.size\n",
    "K = 5\n",
    "steps = 10000\n",
    "\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "R = R_rows\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "alpha = 0.0002 \n",
    "beta = 0.02\n",
    "P, Q = matrix_factorization(R, P, Q, K, steps, verbose=False)\n",
    "print(f'Computed P: {P}')\n",
    "print(f'Computed Q: {Q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10000 steps completed at 896.1545906066895 seconds. Final error: 600.8951675905838<br />\n",
    "Computed P: [[ 1.47815004  1.28952499  1.83660596  1.51649739  1.60369889]\n",
    " [ 1.60384677  1.57248303  1.41599001  1.43887332  1.37809628]\n",
    " [ 1.17154936  1.18381961  2.08597216  1.73877099  1.59331527]\n",
    " [ 1.66968646  1.27915259  1.62334644  1.70098532  1.35716274]\n",
    " [ 1.69106398  1.36013277  1.66209292  1.64966125  1.2517131 ]\n",
    " [ 1.5671748   1.27496652  1.61724517  1.72165933  1.49233743]\n",
    " [ 1.87093831  1.82344903  1.64868914  1.44056611  0.77047785]\n",
    " [ 1.09283558  1.7396331   2.21563333  0.24605293  2.12837725]\n",
    " [ 1.45755035  1.25923598  1.62216181  1.83441417  1.30106972]\n",
    " [ 2.38317662  2.07918878  1.21598366  0.69586391  1.07654876]\n",
    " [ 1.68340854  1.75390384  1.60003676  1.29661807  1.37095238]\n",
    " [ 1.27292078  1.51045777  1.87111258  1.86380874  1.27052078]\n",
    " [ 1.37458142  1.18747103  1.72859737  1.97914437  1.42790576]\n",
    " [ 1.45424295  1.19948916  1.82142714  1.42713104  1.70926799]\n",
    " [ 1.81720618  1.36680623  1.35428705  1.55090396  1.42025479]\n",
    " [ 1.76244275  1.17974857  1.5444533   1.73140141  1.45535017]\n",
    " [ 1.95034435  1.36778176  1.39425955  1.1719843   1.59728768]\n",
    " [ 1.57361984  1.38179055  1.64032634  1.626005    1.39872292]\n",
    " [ 0.26124318  3.20492801  2.00712599 -0.41404923  2.31814372]\n",
    " [ 1.65742575  1.30789391  1.5915646   1.47407491  1.61945972]\n",
    " [ 1.46993305  1.38094748  1.64627054  1.92258716  1.25390068]\n",
    " [ 1.35422005  1.19562375  1.79168213  1.93972607  1.37637275]\n",
    " [ 1.43276741  1.21135836  1.62893694  1.84437448  1.39544826]\n",
    " [ 1.29957149  1.99306449  2.57424982 -1.07860321  2.80989568]\n",
    " [-0.00755112  3.74488646  0.51164251  1.79757794  0.49246488]\n",
    " [ 1.68170093  1.33198654  1.56684659  1.66127065  1.359148  ]\n",
    " [ 1.36282744  2.01277067  1.09675587  2.00214406  1.27185521]\n",
    " [ 1.77019646  1.53355656  1.3779453   1.46569758  1.4370228 ]\n",
    " [ 1.53590044  1.38908149  1.49671647  1.78512817  1.42422501]\n",
    " [ 1.28813198  2.53860613  1.35310984  1.32108563  0.96577947]\n",
    " [ 1.6714027   1.21736642  1.61209293  1.62378407  1.50723697]\n",
    " [ 1.67402673  1.29973518  1.71242428  1.43799705  1.4564015 ]\n",
    " [ 1.17884929  1.91413976  1.78178624  0.96099202  1.85499877]\n",
    " [ 1.59626579  1.22852143  1.69941187  1.49380463  1.60591295]\n",
    " [ 1.46190482  1.21552257  1.69214855  1.82948599  1.39281188]\n",
    " [ 1.6999365   1.16738069  1.57891216  1.7367158   1.39287246]\n",
    " [ 1.00699277  1.63742454  1.8210891   1.49360279  1.63601249]\n",
    " [ 0.35994022  0.04728069 -1.11548956  2.26012476  4.92325195]\n",
    " [ 1.38843044  1.19681061  1.67836394  1.87567042  1.45336849]\n",
    " [ 2.08206979  1.41105147  1.13359532  1.41964063  1.53557556]\n",
    " [ 1.32565643  1.17046914  1.73128798  1.95930223  1.48631052]\n",
    " [ 1.43601724  1.24713017  1.79245637  1.82977601  1.37749198]\n",
    " [ 1.69604129  1.28366871  1.47846068  1.63096783  1.3589279 ]\n",
    " [ 2.76651592  2.1172529   0.60297938  0.30089487  1.25621378]\n",
    " [ 2.1103361   1.26843343  1.12641252  1.26915854  1.40819354]\n",
    " [ 1.73663144  1.28408667  1.59342443  1.50586509  1.46532444]\n",
    " [ 1.65392396  1.24847811  1.4741332   1.6478463   1.50949889]\n",
    " [ 1.41636731  1.95925486  1.08895598  1.96882024  1.24260505]\n",
    " [ 1.50944112  1.49608257  1.00388413  1.91505174  1.68879997]\n",
    " [ 1.37469123  0.98325744  1.94838388  1.61310964  1.9278874 ]\n",
    " [ 1.66335198  1.23299275  1.58701274  1.63606512  1.55436869]\n",
    " [ 2.52649876  1.56030276  0.40177497  0.82377317  1.56243607]\n",
    " [ 0.63645034  1.92196881  2.46678302  0.62970875  2.28536812]\n",
    " [ 1.28265164  1.39562993  1.65625263  1.93581692  1.45499118]\n",
    " [ 1.2927257   1.17072342  1.74911967  1.90675085  1.51180425]\n",
    " [ 1.71529436  1.20883368  1.47031706  1.72698688  1.46547107]\n",
    " [ 1.83839613  2.28738545  0.71299356  1.5175977   1.21127074]\n",
    " [ 1.4085665   1.26057243  1.77826982  1.81052507  1.30703762]\n",
    " [ 1.35387959  1.23904362  1.62953322  1.88599538  1.39336992]\n",
    " [ 2.02446655  1.64766239  1.37126604  1.34484476  1.10347446]\n",
    " [ 1.67703119  1.36812775  1.75047546  1.63242318  1.17165162]\n",
    " [ 1.31021712  1.25104398  1.72822679  1.90934681  1.40217607]\n",
    " [ 1.97182339  1.55773179  1.28322402  1.07677543  1.68575887]\n",
    " [ 1.99277764  1.42946397  1.46953415  1.44192402  1.36891701]\n",
    " [ 2.00481094  1.31856258  1.19144535  1.26140515  1.56093617]\n",
    " [ 1.59419723  2.35626639  1.59144658  1.72538581  0.39801083]\n",
    " [ 1.63163727  1.22303819  1.55897627  1.75443222  1.5202855 ]\n",
    " [ 1.75231122  1.2983764   1.5670942   1.68925037  1.28851736]\n",
    " [ 1.36217546  1.19250823  1.76324396  1.94910968  1.38457693]<br />\n",
    "Computed Q: [[ 0.26256853  0.31826003  0.19269399  0.2548946   0.31079818]\n",
    " [ 0.41582955  0.44059562  0.4225098   0.31117937  0.18844315]\n",
    " [ 0.40559035  0.50987204  0.48570688  0.23120881  0.72237396]\n",
    " ...\n",
    " [ 0.72865626  1.23641647  0.85232934 -0.46132521 -1.92645125]\n",
    " [-0.21263808  1.22464339 -0.75935773  0.88791209 -0.31376417]\n",
    " [ 1.38391384  0.22348793 -0.29069399 -1.1368277   0.53452747]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also tried running with the sample containing 100,000 records for 100 iterations which yielded the error of 20613178.730349187."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_df = df.select('scaled')\n",
    "R_rows = R_df.collect()\n",
    "R_vector = R_rows[0].scaled\n",
    "                               \n",
    "N = R_df.count()\n",
    "M = R_vector.size\n",
    "\n",
    "R = R_rows\n",
    "P = np.random.rand(N,K)\n",
    "Q = np.random.rand(M,K)\n",
    "\n",
    "steps = 100\n",
    "K = 5\n",
    "alpha = 0.0002\n",
    "beta = 0.02\n",
    "\n",
    "P, Q = matrix_factorization(R, P, Q, K, steps, verbose=False)\n",
    "print(f'Computed P: {P}')\n",
    "print(f'Computed Q: {Q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 steps completed at 10507.08092212677 seconds. Final error: 20613178.730349187<br />\n",
    "Computed P: [[ 8.65407765 14.1279773   7.07762428 15.40271375 10.0926271 ]\n",
    " [ 3.39865382  3.16282355  3.79418207  5.77215809  5.32639565]\n",
    " [12.04752889  6.99741921 11.06798678 10.45646698  7.34169844]\n",
    " ...\n",
    " [ 8.76420987  9.51573402  7.96029121  7.95495166  7.36124456]\n",
    " [ 5.24210817  9.74639662 13.89547145 10.56124443 11.61630266]\n",
    " [ 7.95608124  8.37944255  5.88899514 10.49921228 11.47150622]]<br />\n",
    "Computed Q: [[ 0.02430798  0.03765933  0.06134319  0.04210687  0.05937616]\n",
    " [ 0.05061193  0.0442014   0.08177039  0.06038206  0.09940075]\n",
    " [ 0.06389963  0.05478331  0.09862786  0.07791446  0.12322155]\n",
    " ...\n",
    " [ 0.00694229 -0.00590551  0.02178846  0.0018724   0.0413201 ]\n",
    " [ 0.00352796 -0.00716726  0.000926    0.02248234  0.0249154 ]\n",
    " [ 0.00630283  0.01263933  0.00935835  0.00819654  0.01943996]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are still interested in learning more about matrix factorization and factorization machine as we learned how powerful they can be in our research, we decided in the interest of time and resources, we will not pursue them further as our main algorithm for this study. We may still try to incorporate the technique in the feature engineering stage if it makes sense. We concluded that since CTR is a continuous probability that predicts the outcome of a binary action (clicked vs not clicked), Logistic Regression should work well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is by nature a binary classification task, we have decided to build a logistic regression model for the CTR prediction. But before the actual algorithm implementation, we'd like to briefly explain the math behind the model using a representative toy data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Example Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.read_csv('toy.txt', delimiter = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 14 \t# of unique values:  10\n",
      "Column 15 \t# of unique values:  55\n",
      "Column 16 \t# of unique values:  90\n",
      "Column 17 \t# of unique values:  85\n",
      "Column 18 \t# of unique values:  8\n",
      "Column 19 \t# of unique values:  7\n",
      "Column 20 \t# of unique values:  91\n",
      "Column 21 \t# of unique values:  11\n",
      "Column 22 \t# of unique values:  2\n",
      "Column 23 \t# of unique values:  72\n",
      "Column 24 \t# of unique values:  88\n",
      "Column 25 \t# of unique values:  90\n",
      "Column 26 \t# of unique values:  83\n",
      "Column 27 \t# of unique values:  8\n",
      "Column 28 \t# of unique values:  88\n",
      "Column 29 \t# of unique values:  89\n",
      "Column 30 \t# of unique values:  9\n",
      "Column 31 \t# of unique values:  76\n",
      "Column 32 \t# of unique values:  22\n",
      "Column 33 \t# of unique values:  4\n",
      "Column 34 \t# of unique values:  89\n",
      "Column 35 \t# of unique values:  6\n",
      "Column 36 \t# of unique values:  12\n",
      "Column 37 \t# of unique values:  71\n",
      "Column 38 \t# of unique values:  14\n"
     ]
    }
   ],
   "source": [
    "#subsample categorcial feature columns\n",
    "for i in range(14, 39):\n",
    "    print ('Column', i, '\\t# of unique values: ', len(toy_df[i].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a linear/logistic regression, all of the categorical features will be transformed into dummy variables, or one-hot encoded. Since we know that theoretically, we can't solve for a logistic regression with more parameters than observations. We are gonna select the three columns with the lowest number of features for demonstration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(range(13)) + [22, 33, 35]\n",
    "toy_df_demo = toy_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very similar to the `OneHotEncoderEstimator` in MLlib, here for demostration purpose we will use a built-in function in Pandas to get the dummy variables for categorical columns 22, 33, and 35. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df_demo = pd.get_dummies(toy_df_demo, columns = [22, 33, 35], dummy_na = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although NaN should be considered as a separate category, we are excluding them here so that there is no perfect collinearity among the dummy variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd also want to impute our numerical features just like we plan to for our actual algorithm. Here we will impute nulls with the mean of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for imputation\n",
    "impute_values = {}\n",
    "for i in range(1, 13):\n",
    "    impute_values[i] = toy_df_demo[i].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22_7cc72ec2</th>\n",
       "      <th>22_a73ee510</th>\n",
       "      <th>33_5840adea</th>\n",
       "      <th>33_a458ea53</th>\n",
       "      <th>33_b1252a9d</th>\n",
       "      <th>35_49e825c5</th>\n",
       "      <th>35_8ec974f4</th>\n",
       "      <th>35_ad3062eb</th>\n",
       "      <th>35_c0061c6d</th>\n",
       "      <th>35_c9d4222a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>420</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>795</td>\n",
       "      <td>4.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>7</td>\n",
       "      <td>116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.113208</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24720.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.113208</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24201.978947</td>\n",
       "      <td>103.432099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0          1    2    3     4             5           6     7  8      9  \\\n",
       "0  0   1.000000  420  1.0   5.0     37.000000    5.000000   1.0  4    5.0   \n",
       "1  1  14.000000  795  4.0  24.0     36.000000   19.000000  38.0  7  116.0   \n",
       "2  0   5.000000   85  4.0   8.0     12.000000    8.000000  18.0  9   11.0   \n",
       "3  1   2.113208    0  1.0   1.0  24720.000000  357.000000   1.0  1   95.0   \n",
       "4  0   2.113208    5  1.0   6.0  24201.978947  103.432099   0.0  6    6.0   \n",
       "\n",
       "      ...       22_7cc72ec2  22_a73ee510  33_5840adea  33_a458ea53  \\\n",
       "0     ...                 0            1            0            0   \n",
       "1     ...                 0            1            0            0   \n",
       "2     ...                 0            1            0            0   \n",
       "3     ...                 0            1            0            0   \n",
       "4     ...                 1            0            0            0   \n",
       "\n",
       "   33_b1252a9d  35_49e825c5  35_8ec974f4  35_ad3062eb  35_c0061c6d  \\\n",
       "0            1            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            1            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   35_c9d4222a  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df_demo.fillna(value = impute_values, inplace = True)\n",
    "toy_df_demo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to model this sample data with a logistic regression. We are essentially assuming that there are parameters $\\beta_1$ to $\\beta_{22}$ that:\n",
    "\n",
    "$$ y = \\operatorname{Pr}(clicked) = \\frac{\\exp(\\beta_{0} + \\beta_{1} x_1 + \\beta_{2} x_2 + \\dots +\n",
    "    \\beta_{22} x_{22})}{1 + \\exp(\\beta_{0} + \\beta_{1} x_1 + \\beta_{2} x_2 +\n",
    "\\dots + \\beta_{22} x_{22})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to think about this in a matrix form where:\n",
    "\n",
    "$$ \\boldsymbol{X}^T = \\begin{bmatrix} x_{1}  \\ ... \\ x_{m}\\ \\end{bmatrix}$$\n",
    "\n",
    "$$ \\boldsymbol{Y}^T = \\begin{bmatrix} y_{1}  \\ ... \\ y_{m}\\ \\end{bmatrix}$$\n",
    "\n",
    "$$ \\boldsymbol{x_i} = \\begin{bmatrix} 1\\ x_{i1}  \\ ... \\ x_{i22}\\ \\end{bmatrix}$$\n",
    "\n",
    "$$\\boldsymbol{\\theta}^T = \\begin{bmatrix} \\beta_{0}  \\ \\beta_{1} ... \\ \\beta_{22} \\end{bmatrix}$$ \n",
    "\n",
    "And let sigmoid function G(z) be:\n",
    "\n",
    "$$ G(z) = \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get that each prediction: \n",
    "\n",
    "$$y_i = G(x_i \\theta^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for the logistic regression here is defined as: \n",
    "\n",
    "$$ H(x_i) = G(x_i * \\theta^T) $$\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\dot (y_i log(H(x_i)) - (1-y_i) log(1-H(x_i))) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, simply put, we can think of the loss function as the error of our predictions and the process of solving for the $\\beta$'s as to minimize this loss function. In order to do that, we can use the method of gradient descent where we iteratively adjust the coefficients $\\theta$ to the direction that decreases the loss function. And that direction is given to us by the derivative of the loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a derivative of the loss function with respect to $\\theta$, we can get that the gradient is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (H(x_i) - y_i) * x_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the gradient computed, in every iteration, we get the next $\\theta$ by:\n",
    "\n",
    "$$ \\theta_{new} = \\theta - \\alpha * \\frac{\\partial J(\\theta)}{\\partial \\theta_j} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate for the gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homegrown Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare our data to be in the form of X, Y, and $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_0: (100, 22)\n",
      "y: (100, 1)\n"
     ]
    }
   ],
   "source": [
    "#get feature matrix\n",
    "X_0 = toy_df_demo.as_matrix(columns = toy_df_demo.columns[1:])\n",
    "\n",
    "# this will be done in the homegrown algorithm\n",
    "# #augment each x_i with a leading 1 to be multiplied with the bias (beta_0)\n",
    "# X = np.c_[np.ones((X_0.shape[0], 1)), X]\n",
    "\n",
    "print ('X_0:', X_0.shape)\n",
    "\n",
    "#get label vector\n",
    "y = np.array(toy_df_demo[0])[:, np.newaxis]\n",
    "print ('y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then build a class with a public API for our homegrown Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionHomegrown (object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coef_ = None       # weight vector\n",
    "        self.intercept_ = None  # bias term\n",
    "        self._theta = None      # augmented weight vector, i.e., bias + weights\n",
    "                                # this allows to treat all decision variables homogeneously\n",
    "        self.history = {\"cost\": [], \n",
    "                        \"coef\": [], \n",
    "                        \"intercept\": [], \n",
    "                        \"grad\": []}\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Activation function used to map any real value between 0 and 1\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _loss(self, theta, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the total loss\n",
    "        \n",
    "        Args:\n",
    "            theta(ndarray):    current weights\n",
    "            X(ndarray):        train objects\n",
    "            y(ndarray):        answers for train objects\n",
    "        Return:\n",
    "            gradient(ndarray): analytical gradient vector\n",
    "        \"\"\"\n",
    "        pred = np.dot(X, self._theta)\n",
    "        pred_prob = self.sigmoid(pred)\n",
    "        m = X.shape[0]\n",
    "        total_cost = -(1 / m) * np.sum(y * np.log(pred_prob) + (1 - y) * np.log(1 - pred_prob))\n",
    "        return total_cost\n",
    "    \n",
    "    def _grad(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the objective function\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):        train objects\n",
    "            y(ndarray):        answers for train objects\n",
    "        Return:\n",
    "            gradient(ndarray): analytical gradient vector\n",
    "        \"\"\"\n",
    "        pred = np.dot(X, self._theta)\n",
    "        pred_prob = self.sigmoid(pred)\n",
    "        m = X.shape[0]\n",
    "        gradient = (1 / m) * np.dot(X.T, pred_prob - y)\n",
    "                                    \n",
    "        return gradient\n",
    "                                    \n",
    "    \n",
    "    # full gradient descent, i.e., not stochastic gd\n",
    "    def _gd(self, X, y, max_iter, alpha=0.00000005):\n",
    "        \"\"\"\n",
    "        Runs GD and logs error, weigths, gradient at every step\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            max_iter(int):   number of weight updates\n",
    "            alpha(floar):    step size in direction of gradient\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for i in range(max_iter):\n",
    "            self.history[\"coef\"].append(self._theta[1:].copy())\n",
    "            self.history[\"intercept\"].append(self._theta[0].copy())\n",
    "            \n",
    "            total_cost = self._loss(self._theta, X, y)\n",
    "            self.history[\"cost\"].append(total_cost)\n",
    "\n",
    "            # calculate gradient\n",
    "            grad = self._grad(X, y)\n",
    "            self.history[\"grad\"].append(grad)\n",
    "            \n",
    "            # do gradient step\n",
    "            self._theta -= alpha * grad\n",
    "    \n",
    "                                                                         \n",
    "    def fit(self, X, y, max_iter=1000):\n",
    "        \"\"\"\n",
    "        Public API for fitting a linear regression model\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      train objects\n",
    "            y(ndarray):      answers for train objects\n",
    "            max_iter(int):   number of weight updates\n",
    "        Return:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Augment the data with the bias term.\n",
    "        # So we can treat the the input variables and the bias term homogeneously \n",
    "        # from a vectorization perspective\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # initialize to all zeros if the first step\n",
    "        if self._theta is None:\n",
    "            self._theta = np.zeros((X.shape[1], 1))\n",
    "        \n",
    "        # do full gradient descent\n",
    "        self._gd(X, y, max_iter)\n",
    "        \n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "        return self\n",
    "                                    \n",
    "    def pred_prob(self, X):\n",
    "        \"\"\"\n",
    "        Make a prediction\n",
    "\n",
    "        Args:\n",
    "            X(ndarray):      objects\n",
    "        Return:\n",
    "            pred(ndarray):   predictions\n",
    "        \"\"\"\n",
    "        # check whether X has appended bias feature or not\n",
    "        if X.shape[1] == len(self._theta):\n",
    "            pred = self.sigmoid(np.dot(X, self._theta)) \n",
    "        else:\n",
    "            pred = self.sigmoid(np.dot(X, self.coef_) + self.intercept_)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test run the model with our toy sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LogisticRegressionHomegrown at 0x114f5f080>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_homegrown = LogisticRegressionHomegrown()\n",
    "model_homegrown.fit(X_0, y, max_iter=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5303907843968421,\n",
       " 0.5303907093524957,\n",
       " 0.5303906343093572,\n",
       " 0.5303905592674265,\n",
       " 0.5303904842267033,\n",
       " 0.5303904091871874,\n",
       " 0.5303903341488784,\n",
       " 0.5303902591117762,\n",
       " 0.5303901840758808,\n",
       " 0.5303901090411914,\n",
       " 0.5303900340077082,\n",
       " 0.5303899589754308,\n",
       " 0.5303898839443593,\n",
       " 0.5303898089144929,\n",
       " 0.530389733885832,\n",
       " 0.5303896588583759,\n",
       " 0.5303895838321244,\n",
       " 0.5303895088070774,\n",
       " 0.5303894337832346,\n",
       " 0.5303893587605959]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_homegrown.history['cost'][-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.02926689e-04],\n",
       "        [-3.58265957e-03],\n",
       "        [-1.44290625e-03],\n",
       "        [-1.22100928e-04],\n",
       "        [-5.96400667e-05],\n",
       "        [-1.95206393e-03],\n",
       "        [ 1.63364770e-04],\n",
       "        [-7.86334645e-04],\n",
       "        [-3.42227909e-05],\n",
       "        [ 1.28152079e-05],\n",
       "        [-2.69185170e-05],\n",
       "        [ 7.91417797e-05],\n",
       "        [ 4.21741796e-07],\n",
       "        [-3.73038245e-05],\n",
       "        [-1.85500125e-05],\n",
       "        [-4.17051658e-05],\n",
       "        [ 2.98442532e-06],\n",
       "        [ 6.58440593e-06],\n",
       "        [-9.24920832e-08],\n",
       "        [ 1.50593929e-05],\n",
       "        [-6.91500492e-07],\n",
       "        [-1.58791475e-05]]), array([[ 1.02931777e-04],\n",
       "        [-3.58266981e-03],\n",
       "        [-1.44295885e-03],\n",
       "        [-1.22102981e-04],\n",
       "        [-5.96399856e-05],\n",
       "        [-1.95206905e-03],\n",
       "        [ 1.63374076e-04],\n",
       "        [-7.86361404e-04],\n",
       "        [-3.42212607e-05],\n",
       "        [ 1.28159188e-05],\n",
       "        [-2.69187816e-05],\n",
       "        [ 7.91451765e-05],\n",
       "        [ 4.21745847e-07],\n",
       "        [-3.73049654e-05],\n",
       "        [-1.85506968e-05],\n",
       "        [-4.17068064e-05],\n",
       "        [ 2.98465297e-06],\n",
       "        [ 6.58466917e-06],\n",
       "        [-9.24920940e-08],\n",
       "        [ 1.50600493e-05],\n",
       "        [-6.91531301e-07],\n",
       "        [-1.58798037e-05]]), array([[ 1.02936864e-04],\n",
       "        [-3.58268005e-03],\n",
       "        [-1.44301144e-03],\n",
       "        [-1.22105034e-04],\n",
       "        [-5.96399044e-05],\n",
       "        [-1.95207416e-03],\n",
       "        [ 1.63383382e-04],\n",
       "        [-7.86388162e-04],\n",
       "        [-3.42197307e-05],\n",
       "        [ 1.28166297e-05],\n",
       "        [-2.69190463e-05],\n",
       "        [ 7.91485732e-05],\n",
       "        [ 4.21749899e-07],\n",
       "        [-3.73061062e-05],\n",
       "        [-1.85513811e-05],\n",
       "        [-4.17084469e-05],\n",
       "        [ 2.98488062e-06],\n",
       "        [ 6.58493242e-06],\n",
       "        [-9.24921047e-08],\n",
       "        [ 1.50607058e-05],\n",
       "        [-6.91562109e-07],\n",
       "        [-1.58804599e-05]])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_homegrown.history['coef'][-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, after 25000 iterations with a very slow learning rate, we see that the coefficients have nicely converged and a minimum in the loss function has been found. Since the loss function is a convex function by nature, we know that the minimum is a global minumum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalability Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the implementation of the home grown logistic regression, here is roughly how it could scale in a distributed computing framework like spark: \n",
    "\n",
    "> 1. Initialize $\\theta$ and broadcast to all nodes\n",
    "\n",
    "> 2. Mapper - Divide feature + label to different nodes and calculate sum of loss and gradient (without the division of m)\n",
    "\n",
    "> 3. Reducer - Caculate total loss and gradient. Use gradient to update $\\theta$ and repeat step 2\n",
    "\n",
    "This is a highly scalable algorithm implementation besides the limitation of having to broadcast $\\theta$ to all nodes. The ease of parallelization is due the nature of the mathematical operations under the hood - summation of matrix dot products is both **associative** and **commutative**. Due to the this nice nature, there is no concern of a **race condition** where a proper order for results from mappers is needed for desired output. \n",
    "\n",
    "However, as $\\theta$ gets large in high-dimensional data, it can be memory intensive or even impossible to be broadcased. In that case, dimension reduction like PCA should be performed beforehand to reduce the workload of the broadcast. \n",
    "\n",
    "In terms of **time complexity**, this algorithm is also very efficient. The actual time complexity depends on the exact implementation of the matrix dot product. But in a high level, for each iteration, we do one pass over all observations to compute the gradient. Assuming that the implementation of the matrix dot product is at least as efficient as O(mn) where m is the number of row and n is number of column, our algorithm scales linearly with the sample size (that considers both rows and columns). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course of our analysis, supra, we used a number of key course concepts gained from w261: Machine Learning at Scale.  They include but are not limited to Lazy Evaluation, One Hot Encoding, MapReduce Paradigm and Matrix Factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluation is an evaluation strategy in programming to delay the the evaluation of an expression until its value is needed. This is effective in preventing repeated computation of the same expression. Sparks employs this strategy in its evaluation of data stored in the form of RDDs and does not initiate the execution of transformations until an action is triggered. It is quite possible that one has executed several cells in their Jupyter Notebook but the data may have remained as is if the cells executed have not reached any action.<br />\n",
    "We needed to be mindful of this concept for the course of this project as we used Spark heavily throughout the project. It was perhaps even more prevalent during the earlier phases of the project when we were conducting EDA and feature engineering and applying various transformations to the dataset in dissecting and preparing them for subsequent machine learning algorithmic analysis. As the dataset we were working with was relatively large, each transformation takes a long time and we would often try out the transformation on a smaller sample to see how well our machines can handle the workload. Failing to recognize the lazy evaluating nature of of Spark can cause us to incorrectly assess the time and computational resources required in completing the work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is a representation of categorical variables as binary vectors. Most machine learning algorithms are unable to work with non-numerical values as is and hence they need to be converted into a form that is machine learning algorithm-friendly.\n",
    "\n",
    "This usually happens in the below 2 steps:\n",
    "1. Map categorical values to integer values\n",
    "2. Convert each integer value to a binary vector which consists of zero values except the index of the integer marked with a 1\n",
    "\n",
    "In the dataset were given to work with, we identified the first 14 columns to be numerical and the remaining 25 to be categorical. During our featuring engineering phase, we used MLlibâ€™s StringIndexer and OneHotEncoder functions respectively for each step in one hot encoding the categorical features in order to make them compatible with the machine learning algorithms such as Logistic Regression and Factorization Machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
